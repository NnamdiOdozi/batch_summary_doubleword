## Document Metadata

**S/N:** 1  
**Title:** AI Tools for Actuaries — Chapter 11: Deep Generative Models  
**Authors:** Ronald Richman, Mario V. Wüthrich  
**Date of publication:** 2025-11-30  
**Topic:** Deep Generative Models (DGMs)  
**Sub-topic:** Application in actuarial science, including VAEs, GANs, diffusion models, and decoder transformers  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

This chapter introduces **deep generative models (DGMs)**, which go beyond point prediction to model full probability distributions over data. The techniques discussed are:

- **Variational Auto-Encoders (VAEs)**: A latent factor model that uses an encoder to map data to a latent space and a decoder to reconstruct data from latent samples. The model is trained by maximizing the Evidence Lower Bound (ELBO), which balances reconstruction fidelity and regularization against a prior distribution.
- **Generative Adversarial Networks (GANs)**: A two-player minimax game between a generator (which creates synthetic data from noise) and a discriminator (which classifies real vs. fake data). The generator aims to fool the discriminator, while the discriminator improves its classification ability.
- **Denoising Diffusion Models**: A multi-step generative process that learns to reverse a forward noising process. Starting from pure Gaussian noise, the model iteratively denoises to generate realistic samples. The training objective is to predict the noise added at each step.
- **Decoder Transformer Models**: An implicit probability distribution approach that models sequences auto-regressively — predicting each token conditioned on all previous tokens. Models like GPT, BART, and T5 use causal self-attention to ensure temporal causality.

These models are categorized into two broad paradigms: **latent factor approaches** (VAEs, GANs, diffusion models) and **implicit probability distribution approaches** (decoder transformers).

### 2. Code Availability

Yes, implementation code is available in the form of R/Keras scripts embedded within the chapter. The code examples are provided for:

- **VAE implementation** using `tensorflow` and `keras` libraries for the Sports Car dataset.
- **GAN implementation** using a custom `GAN` class derived from TensorFlow/Keras, with separate training steps for generator and discriminator.
- **Diffusion models** and **decoder transformers** are discussed theoretically but no code is provided for these in the chapter.

The code is presented as executable R chunks, indicating that the authors intend for readers to run and experiment with the models. However, no public GitHub repository or external codebase is referenced.

### 3. Learning Type

The learning type is **unsupervised** for all models discussed. DGMs aim to learn the underlying data distribution \( p(X) \) from unlabeled data without explicit target labels. Although some models (like conditional VAEs or GANs) can be adapted for conditional generation, the core framework presented here is unsupervised.

### 4. Dataset

The primary dataset used for demonstration is the **Sports Car dataset**, which contains 475 observations of 5 engineered features derived from car insurance data (e.g., log-transformed ratios of weight, power, torque, engine speed, and cubic capacity). The data is standardized before modeling.

This is a **real-world dataset** from actuarial applications, though it is small and likely preprocessed for educational purposes. No external dataset names (e.g., MNIST, CIFAR) are used, and no large-scale actuarial datasets are referenced.

### 5. Implementation Details

- **Programming language(s):** R (with `tensorflow` and `keras` wrappers)
- **Key libraries/frameworks:**
  - `tensorflow` and `keras` (for building and training neural networks)
  - `MASS` (for multivariate normal sampling)
  - `ggplot2` and `corrplot` (for visualization)
  - `reticulate` (for interfacing with Python/TensorFlow operations in R)

The code is written in R using Keras 2, which suggests compatibility with TensorFlow 2.x. The implementation is pedagogical, with detailed step-by-step construction of models and loss functions.

### 6. Model Architecture

#### Variational Auto-Encoder (VAE)

- **Encoder**: Maps input \( X \) to parameters of a latent Gaussian distribution \( q_\vartheta(Z|X) = \mathcal{N}(\mu_\vartheta(X), \Sigma_\vartheta(X)) \). Implemented as a feedforward neural network with two dense layers (32 units each, SiLU activation) followed by two output layers for mean and log-variance.
- **Decoder**: Maps latent sample \( Z \) to reconstructed data \( X' \). Implemented as a feedforward network with two dense layers (32 units, SiLU) and a linear output layer. Only the mean is modeled (no covariance matrix).
- **Latent Space**: 10-dimensional Gaussian prior \( \pi(Z) = \mathcal{N}(0, I) \).
- **Training Objective**: ELBO, decomposed into reconstruction loss (mean squared error) and KL divergence regularization.

#### Generative Adversarial Network (GAN)

- **Generator**: Takes 10D Gaussian noise and maps to 5D output via three dense layers (15 → 10 → 5 units, tanh/linear activations).
- **Discriminator**: Takes 5D input and outputs a scalar probability via three dense layers (20 → 10 → 1 units, tanh/sigmoid activations).
- **Training**: Alternating updates — discriminator trained to classify real vs. fake, generator trained to fool discriminator. Loss is binary cross-entropy.
- **Trick**: Labels are randomized slightly (±0.05) to stabilize training.

#### Denoising Diffusion Models

- **Forward Process**: Adds Gaussian noise over T steps: \( X_t = \sqrt{\alpha_t} X_0 + \sqrt{1-\alpha_t} \epsilon \), where \( \alpha_t = \prod_{s=1}^t (1-\beta_s) \).
- **Reverse Process**: Trains a neural network \( \epsilon_\vartheta(X_t, t) \) to predict the noise \( \epsilon \) added at step t. Sampling starts from \( X_T \sim \mathcal{N}(0, I) \) and iteratively denoises.
- **Architecture**: Not specified — the chapter notes that diffusion models are typically implemented with U-Net architectures in image tasks, but no code or specific network is provided for actuarial data.

#### Decoder Transformer

- **Architecture**: Auto-regressive model that factorizes \( p(X_{1:T}) = \prod_{t=1}^T p(X_t | X_{1:t-1}) \).
- **Self-Attention**: Uses causal masking to ensure each token attends only to previous tokens. Positional embeddings are static (not learned).
- **Output**: Projects hidden state to vocabulary space, applies softmax with temperature \( \tau \) for calibration.
- **Training**: Teacher forcing — uses ground-truth tokens during training.
- **Inference**: Autoregressive sampling — generates one token at a time.

### 7. Technical Content

Deep generative models (DGMs) represent a paradigm shift from traditional supervised learning in actuarial science. While supervised models predict point estimates (e.g., claim amounts), DGMs aim to learn the full probability distribution \( p(X) \) of the data or joint distributions \( p(Y, X) \). This enables richer applications such as data augmentation, anomaly detection, scenario simulation, and fairness analysis.

The chapter begins by contrasting DGMs with supervised learning, emphasizing that DGMs do not rely on labeled outcomes but instead learn the underlying data-generating process. The goal is to approximate \( p(X) \) from a finite sample \( \{X_i\}_{i=1}^n \), allowing for sampling new instances \( X' \sim p(X) \) or evaluating likelihoods \( p(X) \) for existing data points.

Two main categories of DGMs are introduced: **latent factor approaches** and **implicit probability distribution approaches**. Latent factor models assume an unobserved variable \( Z \) that explains variability in \( X \), with \( p_\vartheta(X) = \int p_\vartheta(X|z) \pi(z) dz \). Implicit models directly learn the conditional distribution \( p_\vartheta(X_{1:T}) = \prod_{t=1}^T p_\vartheta(X_t | X_{1:t-1}) \) without explicit latent variables.

#### Variational Auto-Encoders (VAEs)

VAEs were introduced by Kingma and Welling (2013) and are among the most stable and interpretable DGMs. The model consists of an encoder that maps data \( X \) to a latent distribution \( q_\vartheta(Z|X) \) and a decoder that maps latent samples \( Z \) to data \( p_\vartheta(X|Z) \). The encoder typically outputs mean and log-variance parameters of a Gaussian distribution, while the decoder outputs the mean of a Gaussian (or Bernoulli for binary data).

Training is based on maximizing the Evidence Lower Bound (ELBO), derived via Jensen’s inequality:

\[
\log p_\vartheta(X) \geq \mathbb{E}_{q_\vartheta(Z|X)}[\log p_\vartheta(X|Z)] - D_{KL}(q_\vartheta(\cdot|X) \| \pi)
\]

The first term is the reconstruction loss, encouraging the decoder to reproduce the input. The second term is a KL divergence that regularizes the encoder’s posterior to match the prior \( \pi(Z) \). This balance ensures that the latent space is well-structured and allows for sampling new data.

A key innovation is the **reparameterization trick**, which allows gradients to flow through stochastic nodes. For Gaussian distributions, \( Z = \mu_\vartheta(X) + \Sigma_\vartheta^{1/2}(X) \epsilon \), where \( \epsilon \sim \mathcal{N}(0, I) \) is independent of \( \vartheta \). This enables backpropagation through the encoder.

In the Sports Car example, a VAE with 5 input dimensions, 32 hidden units, and 10 latent dimensions is trained. The model achieves good reconstruction and captures correlations in the data, though the generated samples are less volatile because only the mean (mode) of the Gaussian decoder is used — the covariance matrix is not modeled.

#### Generative Adversarial Networks (GANs)

GANs, introduced by Goodfellow et al. (2014), frame generative modeling as a game between two networks: a generator \( G \) that creates fake data from noise, and a discriminator \( D \) that classifies real vs. fake. The objective is a minimax game:

\[
\min_G \max_D V(D, G) = \mathbb{E}_{X \sim p_{data}}[\log D(X)] + \mathbb{E}_{Z \sim \pi}[\log(1 - D(G(Z)))]
\]

The discriminator maximizes its ability to distinguish real from fake, while the generator minimizes this ability — effectively trying to fool the discriminator. Training alternates between updating the discriminator (with generator weights frozen) and updating the generator (with discriminator weights frozen).

In practice, GANs are notoriously difficult to train due to issues like mode collapse (generator produces limited variety) and vanishing gradients. The chapter implements a simple GAN in R using Keras, with a 10D latent space and 5D output. The discriminator is a 3-layer network (20 → 10 → 1 units), and the generator is 3-layer (15 → 10 → 5 units). Training uses binary cross-entropy loss and label smoothing (adding ±0.05 noise to labels) to stabilize learning.

The generated samples from the GAN are less convincing than VAE samples, though both replicate the correlation structure of the original data. This highlights a common trade-off: GANs can generate sharper samples but are harder to train, while VAEs are more stable but may produce blurrier outputs.

#### Denoising Diffusion Models

Diffusion models, popularized by Song and Ermon (2019) and Ho et al. (2020), generate data by reversing a noising process. The forward process adds Gaussian noise over T steps, transforming data \( X_0 \) into nearly pure noise \( X_T \). The reverse process learns to denoise, starting from \( X_T \sim \mathcal{N}(0, I) \) and iteratively generating \( X_{t-1} \) from \( X_t \).

The training objective is to predict the noise \( \epsilon \) added at step t:

\[
L_{simple}(\vartheta) = \mathbb{E}_{X_0, \epsilon, t}[\| \epsilon - \epsilon_\vartheta(X_t, t) \|_2^2]
\]

This is equivalent to learning the score function (gradient of log-density) and allows for high-fidelity generation, especially in images. However, sampling is slow because it requires T steps (vs. one step in VAEs/GANs).

The chapter notes that diffusion models are rarely applied in actuarial contexts and refers readers to Keras tutorials for implementation. No code or actuarial example is provided, suggesting that this is an emerging area for future research.

#### Decoder Transformer Models

Decoder transformers, such as GPT, model sequences auto-regressively. The joint distribution is factorized as \( p(X_{1:T}) = \prod_{t=1}^T p(X_t | X_{1:t-1}) \). At each step, the model uses self-attention over previous tokens to predict the next token.

The architecture includes:
- **Causal self-attention**: Each token attends only to previous tokens, enforced by a causal mask (setting future attention scores to -∞).
- **Positional embeddings**: Static functions (not learned) to encode token positions.
- **Output layer**: Projects hidden state to vocabulary space, applies softmax with temperature \( \tau \) for calibration.

Training uses **teacher forcing** — feeding ground-truth tokens during training to capture sequential dependencies. Inference is autoregressive: start with a seed token, sample next token, append, and repeat.

These models scale well with data and parameters, enabling large language models (LLMs) that can perform in-context learning (ICL) — generalizing to new tasks with appropriate prompts. Applications include text generation, summarization, translation, and code generation.

In actuarial contexts, decoder transformers could model sequences of claims, policy events, or text-based risk factors. However, the chapter does not provide an actuarial example, focusing instead on theoretical foundations.

#### Comparative Summary

- **VAEs**: Stable, interpretable, good for latent representation learning and reconstruction. Weakness: blurry samples if only mean is modeled.
- **GANs**: Can generate sharp, realistic samples. Weakness: unstable training, mode collapse.
- **Diffusion Models**: High-quality generation, stable training. Weakness: slow sampling, computationally expensive.
- **Decoder Transformers**: Excellent for sequential data, scalable. Weakness: requires large datasets, no direct application shown in actuarial context.

The chapter concludes by emphasizing that DGMs are foundational for modern LLMs and offer powerful tools for actuarial applications beyond point prediction — enabling simulation, risk assessment, and synthetic data generation. While code is provided for VAEs and GANs, diffusion models and transformers are discussed theoretically, indicating areas for future exploration.

Overall, this chapter provides a comprehensive, technically rigorous introduction to DGMs tailored for actuaries, balancing theory with practical implementation. The focus on real-world actuarial data (Sports Car dataset) grounds the concepts in domain-relevant applications, making it accessible to undergraduate STEM students with a background in probability and neural networks.