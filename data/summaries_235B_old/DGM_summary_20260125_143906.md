## Document Metadata

**S/N:** 11  
**Title:** Deep Generative Models  
**Authors:** Ronald Richman, Mario V. Wüthrich  
**Date of publication:** 2025-11-30  
**Topic:** Artificial Intelligence for Actuarial Science  
**Sub-topic:** Deep Generative Modeling Techniques  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

This chapter introduces **Deep Generative Models (DGMs)**, which aim to learn the underlying probability distribution of data rather than merely producing point predictions. Four primary modeling techniques are discussed:

- **Variational Auto-Encoders (VAEs)**: A latent factor approach that uses an encoder to map data to a latent space and a decoder to reconstruct data from latent samples. Training is based on maximizing the Evidence Lower Bound (ELBO), which balances reconstruction fidelity and regularization via KL divergence.
- **Generative Adversarial Networks (GANs)**: An adversarial framework where a generator network creates synthetic data to fool a discriminator network, which tries to distinguish real from fake data. The training involves a minimax game between the two networks.
- **Denoising Diffusion Models**: A multi-step generative process that learns to reverse a forward noising process by estimating noise added at each time step. Sampling proceeds by iteratively denoising pure Gaussian noise to generate structured data.
- **Decoder Transformer Models**: An implicit probability distribution approach that uses auto-regressive factorization to model sequences token-by-token. These include architectures like GPT, BART, and T5, which rely on self-attention with causal masking to ensure temporal causality.

These models are contrasted with traditional supervised learning by their ability to generate full predictive distributions, enabling applications such as data augmentation, anomaly detection, and conditional generation.

### 2. Code Availability

Implementation code is provided within the document for both **VAE** and **GAN** models using the **R programming language** with the `keras` and `tensorflow` libraries. Code snippets include:

- Building encoder and decoder networks for VAEs.
- Defining the ELBO loss function.
- Constructing and training a GAN with alternating updates for generator and discriminator.
- Generating synthetic samples from trained models.

The code is presented in the context of a “Sports car” insurance dataset example. No GitHub repository or external code link is provided; all code is embedded in the chapter.

### 3. Learning Type

The chapter focuses on **unsupervised learning** techniques, although some methods (like conditional VAEs or GANs) can be adapted for **conditional or semi-supervised** settings. The goal is to learn the underlying data distribution \(X \sim P\) or joint distribution \((Y, X) \sim P\), without relying on labeled outcomes. The models are trained using unlabeled data to approximate the true data-generating process.

### 4. Dataset

The primary dataset used for demonstration is a **real-world insurance dataset** called “SportsCars,” which contains features related to car specifications such as weight, engine power, torque, and engine speed. The dataset has 475 samples and 5 features after preprocessing (log-transformed and standardized). The data is used to train both VAE and GAN models to generate synthetic samples that mimic the original distribution.

No synthetic datasets are used; all examples rely on this real-world dataset. The dataset is not publicly named beyond “SportsCars.rda” and is assumed to be part of the authors’ internal repository.

### 5. Implementation Details

- **Programming language(s):** R (with `tensorflow` and `keras` wrappers)
- **Key libraries/frameworks:**
  - `tensorflow` and `keras` for building and training neural networks.
  - `MASS` for generating multivariate normal samples.
  - `ggplot2` and `corrplot` for visualization.
  - `reticulate` for interfacing with TensorFlow’s Python API in GAN implementation.

The implementations are designed for educational purposes and are not optimized for large-scale production use. No GPU acceleration or distributed training is discussed.

### 6. Model Architecture

#### Variational Auto-Encoder (VAE)

- **Encoder**: Maps input \(X\) to parameters of a latent Gaussian distribution \(q_\vartheta(Z|X) = \mathcal{N}(\mu_\vartheta(X), \Sigma_\vartheta(X))\). Implemented as a feedforward neural network (FNN) with two dense layers using `silu` activation.
- **Decoder**: Maps latent sample \(Z\) to parameters of the data distribution \(p_\vartheta(X|Z)\). In the example, only the mean \(m_\vartheta(Z)\) is modeled (not covariance), using a linear output layer.
- **Latent Space**: 10-dimensional Gaussian prior \(\pi(Z) = \mathcal{N}(0, I)\).
- **Reparameterization Trick**: Used to enable backpropagation through stochastic nodes by expressing \(Z = \mu_\vartheta(X) + \Sigma_\vartheta^{1/2}(X) \epsilon\), where \(\epsilon \sim \mathcal{N}(0, I)\).

#### Generative Adversarial Network (GAN)

- **Generator**: Takes a 10-dimensional Gaussian noise vector \(Z \sim \mathcal{N}(0, I)\) and maps it to a 5-dimensional synthetic sample \(X' = G(Z; \vartheta_1)\). Implemented as a 3-layer FNN with `tanh` activations.
- **Discriminator**: Takes a 5-dimensional sample and outputs a scalar probability \(D(X''; \vartheta_2) \in [0,1]\) indicating whether the sample is real. Implemented as a 3-layer FNN with `tanh` and `sigmoid` activations.
- **Training**: Alternating updates — discriminator is trained to classify real vs. fake samples, generator is trained to fool the discriminator. Labels are randomized slightly to improve stability.

#### Denoising Diffusion Models

- **Forward Process**: Adds Gaussian noise over \(T\) steps: \(X_t = \sqrt{\alpha_t} X_0 + \sqrt{1 - \alpha_t} \epsilon\), where \(\alpha_t = \prod_{s=1}^t (1 - \beta_s)\).
- **Reverse Process**: Trains a neural network \(\epsilon_\vartheta(X_t, t)\) to predict the noise \(\epsilon\) added at step \(t\). Loss: \(L_{\text{simple}}(\vartheta) = \mathbb{E}_{X_0, \epsilon, t} \left[ \| \epsilon - \epsilon_\vartheta(X_t, t) \|_2^2 \right]\).
- **Sampling**: Starts from \(X_T \sim \mathcal{N}(0, I)\) and iteratively denoises to \(X_0\).

#### Decoder Transformer Models

- **Architecture**: Auto-regressive model that factorizes joint distribution as \(p_\vartheta(X_{1:T}) = \prod_{t=1}^T p_\vartheta(X_t | X_{1:t-1})\).
- **Self-Attention with Causal Masking**: Ensures each token attends only to previous tokens. Mask sets future attention scores to \(-\infty\).
- **Output Layer**: Projects hidden state to vocabulary space, applies softmax with temperature \(\tau\) for calibration.
- **Training**: Uses “teacher forcing” — ground-truth tokens are fed as context during training.
- **Inference**: Generates tokens one-by-one, sampling or selecting the most probable next token.

### 7. Technical Content

Deep Generative Models (DGMs) represent a paradigm shift from traditional supervised learning, which typically produces point estimates, to methods that model full probability distributions. This capability is critical in actuarial science, where understanding uncertainty, simulating scenarios, and generating synthetic data for stress testing or fairness analysis are essential. The chapter systematically introduces four major DGM families: Variational Auto-Encoders (VAEs), Generative Adversarial Networks (GANs), Denoising Diffusion Models, and Decoder Transformer Models. Each method is grounded in probabilistic modeling and leverages deep neural networks to approximate complex, high-dimensional distributions.

#### Variational Auto-Encoders (VAEs)

VAEs are introduced as a probabilistic extension of autoencoders, designed to learn a latent representation \(Z\) of input data \(X\) such that new samples can be generated by sampling \(Z\) from a prior and decoding it. The model consists of two neural networks: an encoder that maps \(X\) to parameters of a posterior distribution \(q_\vartheta(Z|X)\), and a decoder that maps \(Z\) to parameters of the data distribution \(p_\vartheta(X|Z)\). The key innovation is the use of the Evidence Lower Bound (ELBO) as a tractable objective for training, since the true log-likelihood \(\log p_\vartheta(X)\) is intractable due to the integral over latent variables.

The ELBO decomposes into two terms: a **reconstruction term**, which encourages the decoder to faithfully reconstruct the input from the latent code, and a **regularization term**, which aligns the encoder’s posterior with a predefined prior (typically a standard Gaussian). This balance ensures that the latent space is both informative and well-structured, enabling meaningful sampling.

The **reparameterization trick** is crucial for training VAEs. In the Gaussian case, it allows gradients to flow through the stochastic sampling step by rewriting \(Z = \mu_\vartheta(X) + \Sigma_\vartheta^{1/2}(X) \epsilon\), where \(\epsilon\) is noise independent of the model parameters. This enables efficient optimization via stochastic gradient descent using Monte Carlo approximations (often with just one sample per data point).

In the provided R implementation, the VAE is applied to a 5-dimensional insurance dataset. The encoder and decoder are simple feedforward networks with `silu` and linear activations, respectively. The model is trained to maximize the ELBO, with separate loss functions for reconstruction and KL divergence. After training, synthetic samples are generated by sampling from the prior and passing through the decoder. While the generated samples match the original data’s correlation structure, they exhibit reduced variance because only the mean (mode) of the decoder’s Gaussian output is used — the covariance is not modeled.

#### Generative Adversarial Networks (GANs)

GANs operate on a fundamentally different principle: adversarial training. The generator \(G\) learns to map random noise \(Z\) to synthetic data \(X'\) that mimics real data, while the discriminator \(D\) learns to distinguish real from fake samples. The training objective is a minimax game: the generator minimizes the discriminator’s ability to detect fakes, while the discriminator maximizes its accuracy.

The mathematical formulation is:
\[
\min_G \max_D V(D, G) = \mathbb{E}_{X \sim p_{\text{data}}(x)}[\log D(X)] + \mathbb{E}_{Z \sim \pi(z)}[\log(1 - D(G(Z)))]
\]
This objective is optimized via alternating gradient updates: first, the discriminator is updated with fixed generator weights; then, the generator is updated with fixed discriminator weights. The discriminator’s loss is a binary cross-entropy, while the generator’s loss is designed to maximize the discriminator’s error.

GANs are notoriously difficult to train due to issues like **mode collapse** (where the generator produces only a limited set of outputs) and **vanishing gradients**. The chapter notes that techniques such as label smoothing and careful hyperparameter tuning can mitigate these problems. In the R implementation, the GAN is trained on the same insurance dataset. The generator and discriminator are both 3-layer FNNs with `tanh` activations. After training, the discriminator’s loss stabilizes near \(-\log(0.5)\), indicating it can no longer distinguish real from fake samples — a sign of successful training. Generated samples are visually and statistically similar to the original data, though slightly less convincing than VAE outputs in this case.

#### Denoising Diffusion Models

Diffusion models offer a radically different approach: instead of learning a direct mapping from noise to data, they learn to reverse a forward noising process. The forward process gradually adds Gaussian noise to data over \(T\) steps until it becomes indistinguishable from pure noise. The reverse process learns to denoise step-by-step, starting from pure noise and ending with a structured sample.

The key insight is that the reverse process can be learned by training a neural network to predict the noise added at each step. The loss function is a simple mean squared error between the true noise \(\epsilon\) and the predicted noise \(\epsilon_\vartheta(X_t, t)\). Once trained, sampling proceeds by starting from \(X_T \sim \mathcal{N}(0, I)\) and iteratively applying the learned denoising step to obtain \(X_0\).

Diffusion models are particularly effective for image and audio generation, where their multi-step process leads to high-quality, diverse outputs. However, they are computationally expensive due to the need for many sampling steps. The chapter notes that applications in actuarial science are currently scarce, and no implementation is provided for the insurance dataset.

#### Decoder Transformer Models

Decoder transformers, exemplified by GPT, are designed for sequential data generation. They model the joint distribution of a sequence \(X_{1:T}\) via auto-regressive factorization: \(p_\vartheta(X_{1:T}) = \prod_{t=1}^T p_\vartheta(X_t | X_{1:t-1})\). At each step, the model uses self-attention over previous tokens to predict the next token, with a causal mask ensuring that future tokens are not attended to.

The architecture includes:
- **Self-Attention with Causal Masking**: Each token’s query attends only to previous keys, implemented by adding \(-\infty\) to future attention scores.
- **Positional Embeddings**: Static functions (not learned) that encode token positions.
- **Output Layer**: Projects hidden state to vocabulary space, applies softmax with temperature \(\tau\) for calibration. Higher \(\tau\) flattens the distribution (more uncertainty), lower \(\tau\) sharpens it (more confidence).

Training uses “teacher forcing” — ground-truth tokens are fed as context, ensuring the model learns correct sequential dependencies. Inference proceeds token-by-token, sampling or selecting the next token until an end-of-sequence token is reached.

These models scale well with data and parameter size, enabling large language models (LLMs) that can perform in-context learning — generalizing to new tasks with appropriate prompts. Applications include text generation, summarization, translation, and code generation. While the chapter does not implement a transformer on the insurance dataset, it highlights their relevance for actuarial tasks involving sequential or textual data.

#### Comparative Analysis and Practical Considerations

Each DGM has strengths and weaknesses:
- **VAEs** are conceptually simple, stable to train, and provide probabilistic encodings, but may produce blurry outputs due to the averaging effect of the ELBO.
- **GANs** can generate sharp, realistic samples but are unstable to train and prone to mode collapse.
- **Diffusion Models** produce high-fidelity outputs and are stable to train but are slow due to multi-step sampling.
- **Decoder Transformers** excel at sequential data and scale well but require large datasets and compute resources.

In actuarial applications, VAEs and GANs are most immediately applicable for generating synthetic portfolios or simulating claim scenarios. Diffusion models may be useful for image-based actuarial tasks (e.g., damage assessment), while transformers could model temporal claim patterns or generate synthetic policy text.

The chapter emphasizes that DGMs go beyond supervised learning by learning the full data distribution, enabling richer analyses such as anomaly detection (via likelihood evaluation), data augmentation, and conditional generation. The provided R code demonstrates that these models can be implemented with modest computational resources, making them accessible for educational and exploratory purposes in actuarial science.

In conclusion, DGMs represent a powerful toolkit for actuaries seeking to model uncertainty, simulate scenarios, and generate synthetic data. While challenges remain in training stability and computational cost, the methods presented provide a solid foundation for applying generative modeling to real-world actuarial problems.